\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Scene Text Recognition in the Wild}

\author{Chen-Yu Lee\\
{\tt\small chl260@ucsd.edu}
\and
Phuc Xuan Nguyen\\
{\tt\small pxn002@ucsd.edu}
}

\maketitle
\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
This paper demonstrates an algorithm to tackle the problem of reading text in uncontrolled ``natural'' photos. In contrast to the traditional OCR problem, for which the focus was on scanned pages, scene text presents a number of challenges more commonly associated with general object recognition, including different viewpoints, sizes/scale, locations, fronts, and styles (neon, graffiti). There are two main contributions of this work: The first is the novel character detector using more representative information in the training step and support vector machine based classifier. The second is a more general word recognition system by setting up the dual of a Quadratic Programming problem. We show a significant improvement in accuracy on Street View Text dataset by 10% without explicit guide from lexicon.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

For most people, a visual display of information is the fastest and most direct way to receive external information, through activities such as billboards and neon signs. However, for the visually impaired, it cannot be conveyed via the visual way to obtain information. In this paper, scene text recognition technique can be used in natural environments, so that the visually impaired person can access to the environmental information in texts.

Scene text recognition could also help improve map services. House number and store name recognition helps improve address geocoding. In most countries, very few addresses have associated geographic coordinates. The geocodes of the majority of addresses is therefore usually computed by interpolating the geocodes of neighboring addresses. Such interpolation introduces errors which may be significant, and may result in poor user experience. With the vehicle's location, the house number and the store name, a better map service of the building of interest can be computed in a straightforward way.

\begin{figure}[t]
\begin{center}
%\includegraphics[width=1\linewidth]{fig/p1}
%\includegraphics[width=1\linewidth]{fig/p10}
\end{center}
   \caption{Examples of character detection and word recognition.}
\label{fig:sample1}
\end{figure}

Smith and Learned-Miller~\cite{118} showe that the similarity among characters could be used to improve scene text recognition. They use bottom-up design by starting with hand segmentations of each character in the form of a rectangular bounding box, then the similarity expert performs on all pairs of bounding boxes within one word to higher the confident score of boxes that are the same in both signal and label spaces. This approach required highly rectified candidate bounding boxes (segmented by hand in~\cite{118}) to obtain reliable similarity output.

Neumann and Matas~\cite{120,119} use tradition pipelines for text recognition that their system first performs text localization by sequential selection from the set of Extremal Regions(ERs), and then use character segmentation approach to produce input for traditional Optical Character Recognition (OCR) engine. While this algorithm rely on stability of both text localization and segmentation procedure.

Netzer et al.,~\cite{121} detect and read house numbers from street level photos using unsupervised feature learning method. They show a robust feature learning approach that does not need to use any existing image descriptors and achieve high accuracy on their benchmark. The experiment setting is similar to our goal while they conduct text recognition on total $10$ classes (from $0$ to $9$) and need training dataset that contains around $73,000$ digits.

Mishra et al.,~\cite{111} propose a Conditional Random Field model to perform word recognition from candidate bounding boxes of each character obtained in similar method as our work. They impose top-down cues obtained from a lexicon-based prior and the optimal word representation is obtained by minimizing the energy function corresponding to the random field model.

While progress has been made on cropped word recognition recently, the main challenge still lies on ene-to-end word recognition on the full image due to a great amount of unpredictable false positive objects. Our work and experiment setting are based on~\cite{417} that we propose a more robust and reliable character detector to achieve a better scene text recognition on both cropped word and whole word recognition on full image.

%-------------------------------------------------------------------------
\section{Character Detection}

Each step of character detection will be discussed in detail.

\subsection{Character Detection with SVM}

Like many object recognition approaches, the first step in our algorithm is to detect potential locations of characters in the input image. In order to detect characters in different fonts and view points, we perform multi-scale and multi-aspect ratio for each character via sliding window classification. However, there are 62 categories (26 lowercase, 26 uppercase, and 10 digits) in our problem, we need to choose a suitable classifier in order to handle this large amount of categories efficiently. Multi-class Support Vector Machine (SVM)~\cite{122} is an robust classifier to deal with multi-class problem. In our implementation, we use Histogram of Gradient (HOG) descriptor~\cite{115}.

This sliding window detection method produces many possible locations with different scales and aspect ratios. We call them candidate bounding boxes. However, some of those candidate boxes are not useful for recognizing words in the next step. We eliminate those bounding boxes using the following method.

\subsection{Candidate Re-scoring and NMS}

English alphabet tends to have certain aspect ratios for each character. For example, character `I' is usually thinner than `W' and `l' is thinner than `m'. We then adopt this aspect ratio heuristic to re-score those candidate bounding boxes base on their aspect ratios using normal distribution model:
\[
AC(l_i) = \exp \Bigg( \frac{-(\mu_{a_j} - a_i)^2}{2\sigma_{a_j}^2} \Bigg)
\]
where $\mu_{a_j}$ and $\sigma_{a_j}$ are the mean and variance of the aspect ratio (computed from training data) for character $j$ for a window $l_i$ with aspect ratio $a_i$ and $AC(l_i)$ is the aspect ratio probability value. At testing time, all candidate bounding boxes for each character will be re-scored by multiplying the probability score of normal distribution. Confident score of all candidate bounding boxes with unreliable aspect ratios would be suppressed by the probability value.

We then apply Non-Maximum Suppression (NMS) for each character to address the issue of multiple overlapping detection for each instance of a character. Notice that NMS is performed after aspect ratio pruning because wide candidates may contain other thin candidates while thin candidates are true locations and we do not want NMS first eliminate those true but thin candidates.

\begin{figure}[t]
\begin{center}
\includegraphics[width=1\linewidth]{fig/train_data}
\end{center}
   \caption{Examples of truncated training data. Notice that the internal geometric information still preserved after eliminate information on both sides.}
\label{fig:sample1}
\end{figure}

\begin{figure*}[ht]
\begin{center}$
\begin{array}{cc}
%\includegraphics[width=1\linewidth]{fig/p13_after}
\end{array}$
\end{center}
\caption{Illustration of character detection and word recognition.}
\label{figure:end-to-end}
\end{figure*}

%-------------------------------------------------------------------------
\section{Word Recognition}

The character detection step provides us with a large amount of potential candidate bounding boxes for each character. Our next step is to find words from those candidates and produce reliable word recognition output.

\subsection{Problem setup}

Let $x=(x_{1},...,x_{n})$ be the candidates bounding boxes and $s=(s_{1},...,s_{n})$
be the scores associated with them. We want to find the optimal configuration
that minimizes the following cost function
\begin{eqnarray*}
x^{*} & = & argmin-s^{T}x+x^{T}Ax\\
s.t &  & x_{i}(x_{i}-1)=0\forall i=1...n
\end{eqnarray*}
where A is a cost matrix. The construction of the cost matrix is descirbed in the next section. Given the primal problem, the dual of this problem is derived as followed,
\begin{eqnarray*}
d^{*} & = & max-\frac{1}{2}(s+\lambda)^{T}(A+\mbox{diag}(\lambda))^{\dagger}(s+\lambda)\\
s.t. &  & A+\mbox{diag}(\lambda)\succeq0\\
 &  & s+\lambda\in\Re(A+\mbox{diag(\ensuremath{\lambda}))}
\end{eqnarray*}
The semidefinite programming equivalent of this problem is expressed as followed,

\begin{eqnarray*}
& max & t\\
 & s.t. & \left[\begin{array}{cc}
A+\mbox{diag}(\lambda) & -\frac{1}{4}(s+\lambda)\\
-\frac{1}{4}(s+\lambda) & -t
\end{array}\right]\succeq0
\end{eqnarray*}
The dual of the SDP is

\begin{eqnarray*}
d^{*} & = & \mbox{argmin} tr(AZ)-\frac{1}{2}s^{T}z\\
s.t. &  & diag(Z)=\frac{1}{2}z\\
 &  & \left[\begin{array}{cc}
Z & z\\
z^{T} & 1
\end{array}\right]\succeq0
\end{eqnarray*}
We treat this SDP problem as a relaxation of the primal problem. After obtaining the solution for the SDP, we search across the values of z as thesholds to minimize the original cost function.
\subsection{Construction of cost matrices}
We construct the cost matrix from three factors: collisions, bigram probability, and the color similarities. Mathematically, the cost matrix is defined as,

\begin{eqnarray*}
A=-\alpha B+\beta C+\gamma S
\end{eqnarray*}

where B, C, and S matrices represent bigram probability, collision, and color similarity, while $\alpha$, $\beta$, and $\gamma$ are the weighting factors.
Collision was factored in the cost matrix as the intuition provides that correct bounding boxes often have little overlaps with another. Figure ? provides an example where the collision distinction helps in recognition. Mathematically, the collision matrix is constructed as 

\begin{eqnarray*}
C_{ij}=\frac{area_{i}\cap area_{j}}{area_{i}+area_{j}}
\end{eqnarray*}

We construct a bigram model from our lexicon, consisted of 427 words. We want to dampen the bigram effects if the bounding boxes are far away from another. So we multiply the bigram score with the inverse of the Euclidean distance between the bounding box. The bigram matrix is mathematically defined as
\[
B_{ij}=\begin{cases}
\frac{P(l_{i}|l_{j})}{d_{ij}} & x_{i}>x_{j}\\
0 & otherwise
\end{cases}
\]

where $P(l_{i}|l_{j})$ is the probability that the letter $l_{j}$ is followed by $l_{i}$ and $d_{ij}$ is the Euclidean between the bounding boxes. The similarity matrix is constructed by computing the pairwise $\chi^2$ distance the color histogram of each bounding box. We use 15 bins for each color channel.

\begin{figure}[]
	\subfigure[{}]{ \includegraphics[width=1\linewidth]{fig/bigram}  \label{fig:bookstein1} }
	\subfigure[{}]{ \includegraphics[width=1\linewidth]{fig/collision}  \label{fig:bookstein2} }
	\subfigure[{}]{ \includegraphics[width=1\linewidth]{fig/color}  \label{fig:bookstein3} }
\caption{Examples describing the effects of the factors in the cost matrix. Picture (a) shows an example where the bigram would break the tie between both plausible explanation for the words. The bigram would weight the chance of the letter 'D' followed by the letter 'O' high than the digit '9'. Example (b) shows the effects of minizing the overlaps would lead the algorithm to choose 'DO' over 'DK' or 'KO'. Example (c) shows the benifits of the similarity matrix. The bounding box containing the letter 'D' is closer in term of color histogram distance to the bounding box containing the letter 'O' than the letter 'P'. }\label{fig:bookstein}
\end{figure}

%-------------------------------------------------------------------------
\section{Experiment}
\subsection{Dataset}

We use the Street View Text (SVT)~\cite{417} dataset in our experiments. The SVT dataset contains images taken from Google Map Street View. The words in SVT images come from local business signs and have high degree of variability in appearance and resolution. There are total $647$ cropped word images from the SVT testing set.

\subsection{Character Detection}

There are many descriptors can be used for character recognition, and we choose the HOG feature as our descriptor that it outperform the others in~\cite{117} because it preserves better geometric information. We densely compute HOG features with a cell size of $8 \times 8$ using $8$ bins after resizing each image to $48 \times 48$ windows. This character detector is trained on ICDAR 2003 dataset, char74K, and our synthetic data using a one-againt-one SVM classifier with an RBF kernel, where the synthetic data contains about 10,000 images for 62 classes using 40 fonts and for each image we add some amount of Gaussian noise, and apply a random affine deformation.

We then perform sliding window based character detection for multi-scale and multi-aspect ratio for every location in the input image. The candidate bounding boxes obtained by SVM classifier are then pruned by normal distribution model with a probability less than $0.4$. NMS is performed on the remaining bounding boxes to avoid wrong elimination. These two pruning steps are simply but efficient to discard noisy candidate while still preserve most true positive candidates for the next word recognition step.

\subsection{Cropped Word Recognition}

Using the character detection and recognition described in previous section, we evaluate our method on the cropped region of the SVT-WD data set. We empirically find that $\alpha=.4$, $\beta=.4$, and $\gamma=.2$ give the best performance. We achieve the accuracy of 52.2\% on the test set. While our result is worse than the original method, we did not rely much on the lexicon. Our algorithm only leverages from the bigram built from the provided lexicons. Even so, we train our bigram model on the whole lexicon of 427 words, not just 50 words as the original method describes. Due to the restriction on time, we are not able to fully search over our parameter and fully discover the weighting factors in the cost matrices. As another metric for our evaluation, we compute the Levenshtein's distance between the prediction and the ground truth. The average edit distance is 2.484. The average length of the words in the lexicon is 5.818.

\begin{table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Approach & Accuracy \\
\hline\hline
Previous work~\cite{417}  & 0.56 (364 words) \\
Multi-scale + Multi-ratio & 0.60 (390 words)\\
Multi-scale + Multi-ratio + TT & 0.66 (427 words)\\
\hline
\end{tabular}
\end{center}
\caption{Cropped Word Recognition Accuracy: A comparison of proposed method to PLEX in~\cite{417}. We improve the accuracy by $10\%$ in SVT-WORD dataset.}
\label{table:compare}
\end{table}

\subsection{Results and Discussion}

We evaluate our methods on the challenging SVT dataset, which contains words in different viewpoints, sizes/scale, locations, fronts, and styles with low resolution or low contract conditions. We observe that our proposed method outperform~\cite{117} in cropped word recognition by $10\%$.

We study the effect of multi-ratio of width and height. Original work in~\cite{417} uses fixed aspect ratio for sliding window approach. However, we observe that different characters tend to have different ratios between width and height. In order to leverage this feature, multi-ratio of width and height for sliding window is performed in our algorithm, and the word recognition accuracy is higher by $4\%$ as in Table~\ref{table:compare}. The reason could be that our multi-ratio approach can better capture true character bounding boxes in natural image with high variability.

We also analyze the effect of truncated training data. Random Fern classifier is train on square training data that contain some noise on both sides in~\cite{417} .However, in many real street view cases, characters are tightly connected and it is difficult to contain all possible noise in training step. Therefore, we train our character detector on truncated training data by discarding $0.3$ width information on both sides to capture the internal geometric information. The training error is higher than before by using truncated training data but surprisingly the word recognition accuracy is higher by $6\%$ more and achieve total $10\%$ higher (Table~\ref{table:compare}) in cropped word recognition accuracy than in~\cite{417}. The result shows that we only need the internal information to capture characters in natural street view image and we can avoid the labor of collecting all possible combinations for the noise in both sides in training step. In the reminder of the paper we will refer to the truncated training procedure as ``TT''

\begin{figure}
\begin{center}$
\begin{array}{cc}
%\includegraphics[width=1\linewidth]{fig/p9} \\
%\includegraphics[width=1\linewidth]{fig/p11} \\
%\includegraphics[width=1\linewidth]{fig/p12} \\
\end{array}$
\end{center}
\caption{Experiment results on SVT-WORD dataset.}
\label{figure:result1}
\end{figure}

%-------------------------------------------------------------------------
\section{Conclusion}

Our work establish a new approach to improve traditional sliding window based character detector and a novel way to select useful training data by truncating each character on both sides and further extract more representative geometric information for character classifier. We show that we can outperform original work in~\cite{417} for both cropped word recognition and ene-to-end scene text recognition on full images, and contribute to better text recognition in the natural images.


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
